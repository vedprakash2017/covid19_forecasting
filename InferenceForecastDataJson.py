def ifd():
    # -*- coding: utf-8 -*-
    """InferenceForecastDataJson.ipynb

    Automatically generated by Colaboratory.

    Original file is located at
        https://colab.research.google.com/drive/1opRO-vzDK9vS0q20G1zby3DvQqSki3Ou
    """

    import numpy as np
    import networkx as nx
    import pandas as pd
    import matplotlib.pyplot as plt
    from matplotlib.pyplot import figure
    import random
    import requests
    import lxml.html as lh
    import re
    import math
    import cvxpy as cp
    import warnings
    import itertools
    # warnings.filterwarnings("ignore")
    import statsmodels.api as sm
    from datetime import datetime, timedelta, date

    # districts in karnataka 
    states = {                          #1:'Andaman and Nicobar Islands',
    2:'Andhra Pradesh',
    3:'Arunachal Pradesh',
    4:'Assam',
    5:'Bihar',
    6:'Chandigarh',
    7:'Chhattisgarh',
    # 8:'Daman and Diu',
    9:'Dadra and Nagar Haveli and Daman and Diu',
    10:'Delhi',
    11:'Goa',
    12:'Gujarat',
    13:'Haryana',
    14:'Himachal Pradesh',
    15:'Jammu and Kashmir',
    16:'Jharkhand',
    17:'Karnataka',
    18:'Kerala',
    19:'Ladakh',
    20:'Madhya Pradesh',
    21:'Maharashtra',
    22:'Manipur',
    #23:'Meghalaya',
    #24:'Mizoram',
    25:'Odisha',
    26:'Puducherry',
    27:'Punjab',
    28:'Rajasthan',
    29:'Tamil Nadu',
    30:'Telangana',
    31:'Tripura',
    32:'Uttar Pradesh',
    33:'Uttarakhand',
    34:'West Bengal'
        
    }

    #edges represents the adjacent districts
    edges = [
             (states[2],states[30]),(states[2],states[17]),(states[2],states[29]),(states[2],states[25]),(states[2],states[7]),
             (states[3],states[4]),
             (states[4],states[22]),(states[4],states[31]),(states[4],states[34]),
             (states[5],states[16]),(states[5],states[32]),(states[5],states[34]),
             (states[6],states[27]),(states[6],states[13]),
             (states[7],states[16]),(states[7],states[20]),(states[7],states[25]),(states[7],states[32]),(states[7],states[21]),(states[7],states[30]),
             (states[9],states[21]),(states[9],states[12]),
             (states[10],states[13]),(states[10],states[32]),
             (states[11],states[21]),(states[11],states[17]),
             (states[12],states[28]),(states[12],states[21]),(states[12],states[20]),
             (states[13],states[27]),(states[13],states[32]),(states[13],states[33]),(states[13],states[28]),(states[13],states[14]),
             (states[14],states[15]),(states[14],states[19]),(states[14],states[27]),(states[14],states[33]),(states[14],states[32]),
             (states[15],states[19]),(states[15],states[27]),
             (states[16],states[32]),(states[16],states[34]),(states[16],states[25]),
             (states[17],states[21]), (states[17],states[18]), (states[17],states[30]),(states[17],states[29]),
             (states[18],states[29]),
             (states[20],states[28]),(states[20],states[21]),(states[20],states[32]),
             (states[21],states[30]),
             (states[25],states[34]),
             (states[26],states[29]),
             (states[27],states[28]),
             (states[28],states[32]),
             (states[32],states[33])
            ]

    G = nx.Graph()
    for i in range(2,34):
      if(i!= 8 and i!= 23 and i!= 24):
        G.add_node(states[i])
    G.add_edges_from(edges)

    A = nx.adjacency_matrix(G)
    A = A.todense()
    A = np.array(A)
    for i in range(len(A)):
      for j in range(len(A[0])):
        if(i == j):
          A[i][j] += 1

    # Fetching the state wise historical infection data

    json_df = pd.read_json('https://api.covid19india.org/states_daily.json')
    state_codes = [*json_df['states_daily'][0].keys()]
    state_codes.remove('status')
    state_codes.remove('date')
    data = pd.DataFrame(columns=['active','confirmed','recovered','deceased', 'cum_active', 'cum_confirmed','cum_recovered','cum_deceased','date','state_code'])
    for state_code in state_codes:
        cum_conf = 0
        cum_active = 0
        cum_rec = 0
        cum_dec = 0
        for j in [*json_df.index]:
            if(j%3==0):
                if(json_df['states_daily'][j][state_code]):
                    confirmed = int(json_df['states_daily'][j][state_code])
                    cum_conf+=confirmed
                else:
                    confirmed = 0
            elif(j%3==1):
                recovered = int(json_df['states_daily'][j][state_code])
                cum_rec += recovered
            elif(j%3==2):
                deceased = int(json_df['states_daily'][j][state_code])
                cum_dec += deceased
                active = confirmed - recovered - deceased
                cum_active += active
                date = datetime.strftime(datetime.strptime(json_df['states_daily'][j]['date'], '%d-%b-%y'), '%Y-%m-%d')
                sc = state_code
                a = [active, confirmed, recovered, deceased, cum_active, cum_conf, cum_rec, cum_dec, date, sc]
                data = data.append(dict(zip(data.columns, a)), ignore_index=True)
    json_df = pd.read_json('https://api.covid19india.org/state_district_wise.json')
    state_code_df = pd.DataFrame(columns=['state','state_code'])
    for column in json_df.columns:
        s = column
        sc = json_df[column]['statecode'].lower()
        row = [s, sc]
        state_code_df = state_code_df.append(dict(zip(state_code_df.columns, row)), ignore_index=True)
    state_code_df = state_code_df.append(dict(zip(state_code_df.columns, ['Daman and Diu', 'dd'])), ignore_index=True)
    state_code_df = state_code_df.append(dict(zip(state_code_df.columns, ['Total', 'tt'])), ignore_index=True)
    data = data.merge(state_code_df, how='left').drop_duplicates()

    # Fetching state wise population data

    json_df = pd.read_json('https://api.covid19india.org/state_test_data.json')
    state_test_data = pd.DataFrame()
    for j in json_df.index:
        dictionary = json_df['states_tested_data'][j]
        df = pd.DataFrame(dictionary, index=[j])
        state_test_data = pd.concat([state_test_data, df])
    state_pop_data = state_test_data[['state','populationncp2019projection']].drop_duplicates().dropna(how='any',axis=0)
    state_pop_data = state_pop_data[state_pop_data['populationncp2019projection']!='']
    state_pop_data = state_pop_data.reset_index().drop(['index'], axis = 1)
    state_pop_data['populationncp2019projection'] = (state_pop_data['populationncp2019projection']).astype(int)


    state_testing_data = state_test_data[['state','totaltested','updatedon']].drop_duplicates().dropna(how='any',axis=0)
    state_testing_data= state_testing_data[state_testing_data['totaltested']!='']
    state_testing_data= state_testing_data.reset_index().drop(['index'], axis = 1)
    state_testing_data = state_testing_data.rename(columns={"updatedon":"date"})
    state_testing_data['date'] = state_testing_data.apply(lambda x: datetime.strftime(datetime.strptime(x['date'], '%d/%m/%Y'), '%Y-%m-%d'), axis=1)
    state_testing_data['totaltested'] = (state_testing_data['totaltested']).astype(int)

    data = pd.merge(data, state_pop_data, on='state', how='left')

    data = pd.merge(data, state_testing_data, on=['state','date'], how='left')

    data = data[data['state'] != 'Andaman and Nicobar Islands']
    data = data[data['state'] != 'Daman and Diu']
    data = data[data['state'] != 'Meghalaya']
    data = data[data['state'] != 'Mizoram']
    data = data[data['state'] != 'Lakshadweep']
    data = data[data['state'] != 'Sikkim']
    data = data[data['state'] != 'Nagaland']
    data = data[data['state'] != 'Total']
    data = data[data['state'] != 'State Unassigned']
    data = data[data['date'] >= '2020-04-10']

    for s in data['state'].unique():
        data_subset = data[data['state']==s]
        data.loc[data_subset.index,'totaltested'] = round(data_subset['totaltested'].interpolate(method ='linear', limit_direction ='backward'))
        data_subset = data[data['state']==s]
        data.loc[data_subset.index,'totaltested'] = round(data_subset['totaltested'].interpolate(method ='linear', limit_direction ='forward'))

    data_subset = data[data['state']=='Andhra Pradesh']
    ratio = data_subset['totaltested']/data_subset['cum_confirmed']
    data_subset = data[data['state']=='Telangana']
    data.loc[data_subset.index,'totaltested'] = [round(i) for i in list(data_subset['cum_confirmed'].values * ratio.values)]

    for s in data['state'].unique():
        data_subset = data[data['state']==s]
        dailytest = [0]
        dailytest.extend(data_subset['totaltested'].values[1:]-data_subset['totaltested'].values[:-1])
        data.loc[data_subset.index,'dailytested'] = dailytest

    data.loc[data[data['dailytested']<0].index, 'dailytested'] = 0
    data['dailytested'] = data['dailytested'].replace({0.0:np.nan})

    for s in data['state'].unique():
        data_subset = data[data['state']==s]
        data.loc[data_subset.index,'dailytested'] = round(data_subset['dailytested'].interpolate(method ='linear', limit_direction ='backward'))
        data_subset = data[data['state']==s]
        data.loc[data_subset.index,'dailytested'] = round(data_subset['dailytested'].interpolate(method ='linear', limit_direction ='forward'))

    b=0.75

    data.loc[data.index,'alpha'] = (data.loc[data.index,'populationncp2019projection']/data.loc[data.index,'totaltested'])**b

    for s in data['state'].unique():
        data_subset = data[data['state']==s]
        daily_active_array = [0.0]
        actual_active_array = [0.0]
        daily_rec_array = [0.0]
        actual_rec_array = [0.0]
        for k,_  in data_subset[1:].iterrows():
            if(data.loc[k,'cum_active']!=0):
                daily_rec = (data.loc[k,'recovered']+data.loc[k,'deceased'])*actual_active_array[-1]/data.loc[k,'cum_active']       
            else:
                daily_rec = data.loc[k,'recovered']+data.loc[k,'deceased']

            if(data.loc[k,'confirmed']!=0.0):
                z = data.loc[k,'dailytested']
                c = data.loc[k,'confirmed']
                Dr = (1.0 - data.loc[k,'alpha'] + data.loc[k,'alpha']*z/c)
                daily_active = data.loc[k,'populationncp2019projection']/Dr - daily_rec       
            else:
                daily_active = data.loc[k,'active']

            daily_rec_array.append(daily_rec)    
            daily_active_array.append(daily_active)
            actual_active_array.append(sum(daily_active_array))
            actual_rec_array.append(sum(daily_rec_array))
        data.loc[data_subset.index,'daily_active'] = [round(j) for j in daily_active_array]
        data.loc[data_subset.index,'actual_active'] = [round(j) for j in actual_active_array]
        data.loc[data_subset.index,'daily_rec'] = [round(j) for j in daily_rec_array]
        data.loc[data_subset.index,'actual_rec'] = [round(j) for j in actual_rec_array]

    data = data[data['date']>='2020-04-11']

    # Historical data duration

    Time = len(data[data['state']=='Maharashtra'])

    S = np.zeros((Time,A.shape[0]))
    X = np.zeros((Time,A.shape[0]))
    R = np.zeros((Time,A.shape[0]))
    DR = np.zeros((Time,A.shape[0]))
    DX = np.zeros((Time,A.shape[0]))

    per = 1

    # Calculating S,X,R,DX,DR

    for i in range(S.shape[0]):
        for j in range(S.shape[1]):
            pop = [*data[data['state']==[*states.values()][j]]['populationncp2019projection']][0]/per
            X[i][j] = np.array(data[data['state']==[*states.values()][j]]['actual_active'])[i]/pop
            R[i][j] = np.array(data[data['state']==[*states.values()][j]]['actual_rec'])[i]/pop
            S[i][j] = (1.0-X[i][j]-R[i][j])
            DR[i][j] = np.array(data[data['state']==[*states.values()][j]]['daily_rec'])[i]/pop
            DX[i][j] = np.array(data[data['state']==[*states.values()][j]]['daily_active'])[i]/pop

    X = np.array(X)
    S = np.array(S)
    R = np.array(R)
    DX = np.array(DX)
    DR = np.array(DR)

    h = 1

    forecast_time = Time+15

    # Function to get beta and gamma values and use it for forecasting

    def get_params(start_slice, end_slice, param):
        X1 = X[start_slice:end_slice]
        S1 = S[start_slice:end_slice]
        R1 = R[start_slice:end_slice]
        DR1 = DR[start_slice:end_slice]
        DX1 = DX[start_slice:end_slice]
        beta = cp.Variable(A.shape, nonneg = True)
        gamma = cp.Variable(A.shape, diag = True)
        constraints = []
        
        for i in range(A.shape[0]):
            for j in range(A.shape[0]):     
                if (A[i][j] == 0):
                    constraints += [
                        beta[i][j] == 0.0,                
                    ]
        cost = cp.sum_squares((DX1 + DR1) - cp.multiply(S1, (X1 * beta.T))) + cp.sum_squares(DR1 - (X1*gamma))
        objective = cp.Minimize(cost)
        prob = cp.Problem(objective, constraints)
        prob.solve()
        return(get_forecast(np.array(beta.value),gamma.value.data[0],end_slice,param))

    # Function to forecast infection using beta and gamma values and Deterministic Method

    def get_forecast(B, Y, end_slice,param):
        S2 = np.zeros((forecast_time,A.shape[0]))
        X2 = np.zeros((forecast_time,A.shape[0]))
        R2 = np.zeros((forecast_time,A.shape[0]))
        for j in range(end_slice):
            for i in range(A.shape[0]):
                S2[j][i] = S[j][i]
                X2[j][i] = X[j][i]
                R2[j][i]= R[j][i]
        for i in range(A.shape[0]):
            for k in range(end_slice,forecast_time):
                a = 0
                j =0
                while(j<len(A)):
                    a += B[i][j]*X2[k-1][j]
                    j = j+1
                S2[k][i] = S2[k-1][i] + h*(-(S2[k-1][i]*a))
                X2[k][i] = X2[k-1][i] + h*(S2[k-1][i]*a - Y[i]*X2[k-1][i])
                R2[k][i] = R2[k-1][i] + h*Y[i]*X2[k-1][i]
        if(param=='S'):
            return(S2)
        elif(param=='X'):
            return(X2)
        elif(param=='R'):
            return(R2)

    date = list(data['date'].unique())
    for i in range(1,forecast_time-Time+1):
        d = datetime.strptime(max(data[data['state']=='Maharashtra']['date']),'%Y-%m-%d')+timedelta(i)
        date.append(datetime.strftime(d,'%Y-%m-%d'))
    date = pd.DataFrame(date, columns=['date'])

    state_df = pd.DataFrame(data['state'].unique(),columns=['state'])

    state_df['tmp'] = 1
    date['tmp'] = 1

    state_date = pd.merge(date, state_df, on='tmp')
    state_date = state_date.drop(['tmp'], axis=1)

    InferenceData = pd.DataFrame(columns=['state','date','active','recovered'])

    InferenceData = pd.merge(InferenceData, state_date, on=['date','state'], how='right')

    activeInference = get_params(Time-7, Time, 'X')
    recoveredInference = get_params(Time-7, Time, 'R')
    for i in range(A.shape[0]):
      InferenceSubset = InferenceData[InferenceData['state']==[*states.values()][i]]
      pop = [*state_pop_data[state_pop_data['state']==[*states.values()][i]]['populationncp2019projection']][0]
      InferenceData.loc[InferenceSubset.index,'active'] = [round(j) for j in activeInference[:,i]*pop]
      InferenceData.loc[InferenceSubset.index,'recovered'] = [round(j) for j in recoveredInference[:,i]*pop]

    InferenceData.to_json('InferenceData.json',orient='records')

    # train-> train dataset
    # test-> test dataset

    def get_arima_forecast(train):
        warnings.filterwarnings("ignore")
        p = d = q = range(0, 3)
        pdq = list(itertools.product(p, d, q)) 
        min=9999
        minParam = (0, 0, 0)
        for param in pdq:
            try:
                mod=sm.tsa.statespace.SARIMAX(train,
                                                order=param,
                                                seasonal_order=(0,0,0,12),
                                                enforce_stationarity=False,
                                                enforce_invertibility=False)
                results = mod.fit()
                if(results.aic<min):
                    min=results.aic
                    minParam = param
            except:
                continue
        mod=sm.tsa.statespace.SARIMAX(train,
                                            order=minParam,
                                            seasonal_order=(0,0,0,12),
                                            enforce_stationarity=False,
                                            enforce_invertibility=False)
        results = mod.fit(disp=-1)
        return(results.predict(start=0, end=len(train)+14))

    date = []
    for i in range(1,forecast_time-Time+1):
        d = datetime.strptime(max(data[data['state']=='Maharashtra']['date']),'%Y-%m-%d')+timedelta(i)
        date.append(datetime.strftime(d,'%Y-%m-%d'))
    date = pd.DataFrame(date, columns=['date'])

    state_df = pd.DataFrame(data['state'].unique(),columns=['state'])

    state_df['tmp'] = 1
    date['tmp'] = 1

    state_date = pd.merge(date, state_df, on='tmp')
    state_date = state_date.drop(['tmp'], axis=1)

    predicted_data = pd.DataFrame(columns=data.columns)

    predicted_data = pd.merge(predicted_data, state_date, on=['date','state'], how='right')

    for i in range(A.shape[0]):
        s = [*states.values()][i]
        predicted_subset = predicted_data[predicted_data['state']==s]
        x = get_arima_forecast(data[data['state']==s]['totaltested'])

        pop = data[data['state']==s]['populationncp2019projection'].values[0]
        predicted_data.loc[predicted_subset.index, 'populationncp2019projection'] = pop
        predicted_data.loc[predicted_subset.index, 'state_code'] = data[data['state']==s]['state_code'].values[0]

        total_tested = np.array([round(i) for i in x[-15:].values])
        predicted_data.loc[predicted_subset.index, 'totaltested'] = total_tested
        daily_tested = [total_tested[0] - [*data[data['state']==s][-1:]['totaltested']][0]]
        daily_tested.extend(total_tested[1:]-total_tested[:-1])
        predicted_data.loc[predicted_subset.index, 'dailytested'] = daily_tested

        alpha = []
        alpha.extend((pop/predicted_data.loc[predicted_subset.index, 'totaltested'].values)**b)
        alpha = np.array(alpha)
        predicted_data.loc[predicted_subset.index, 'alpha'] = alpha    

        actual_active = np.array([round(j) for j in (get_params(Time-8, Time-1, 'X')[Time:Time+15,i]*pop)])
        predicted_data.loc[predicted_subset.index, 'actual_active'] = actual_active
        daily_active = [actual_active[0] - [*data[data['state']==s][-1:]['actual_active']][0]]
        daily_active.extend(actual_active[1:]-actual_active[:-1])
        daily_active = np.array(daily_active)
        predicted_data.loc[predicted_subset.index, 'daily_active'] = daily_active
        
        actual_rec = np.array([round(j) for j in (get_params(Time-8, Time-1, 'R')[Time:Time+15,i]*pop)])
        predicted_data.loc[predicted_subset.index, 'actual_rec'] = actual_rec    
        daily_rec = [actual_rec[0] - [*data[data['state']==s][-1:]['actual_rec']][0]]
        daily_rec.extend(actual_rec[1:]-actual_rec[:-1])
        daily_rec = np.array(daily_rec)
        predicted_data.loc[predicted_subset.index, 'daily_rec'] = daily_rec
        confirmed = predicted_data.loc[predicted_subset.index, 'dailytested']/((1.0/alpha)*((pop/(daily_active+daily_rec))-1.0)+1.0)
        confirmed = np.array([round(j) for j in confirmed])
        predicted_data.loc[predicted_subset.index, 'confirmed'] = confirmed

        cum_active = [[*data[data['state']==s][-1:]['cum_active']][0]]
        cum_recovered = [*data[data['state']==s][-1:]['cum_recovered']][0]
        cum_confirmed = [*data[data['state']==s][-1:]['cum_confirmed']][0]
        for index, row in predicted_subset.iterrows():
            predicted_data.loc[index,'recovered'] = round(cum_active[-1]*predicted_data.loc[index,'daily_rec']/predicted_data.loc[index,'actual_active'])
            cum_active.append(cum_active[-1]+predicted_data.loc[index,'confirmed']-predicted_data.loc[index,'recovered'])
            predicted_data.loc[index,'cum_active'] = cum_active[-1]
            cum_confirmed+=predicted_data.loc[index,'confirmed']
            predicted_data.loc[index,'cum_confirmed'] = cum_confirmed
            cum_recovered+=predicted_data.loc[index,'recovered']
            predicted_data.loc[index,'cum_recovered'] = cum_recovered
        predicted_data.loc[predicted_subset.index,'active'] = np.array(cum_active[1:]) - np.array(cum_active[:-1])

    ForecastData = predicted_data[['state','date','cum_confirmed','cum_active','cum_recovered','totaltested']]

    #-- For thousand seperation
    ForecastData['cum_confirmed'] = ForecastData['cum_confirmed'].apply(lambda x : "{:,}".format(int(x)))
    ForecastData['cum_active'] = ForecastData['cum_active'].apply(lambda x : "{:,}".format(int(x)))
    ForecastData['cum_recovered'] = ForecastData['cum_recovered'].apply(lambda x : "{:,}".format(int(x)))
    ForecastData['totaltested'] = ForecastData['totaltested'].apply(lambda x : "{:,}".format(int(x)))
    #--

    ForecastData.to_json('ForecastData.json',orient='records')